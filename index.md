## About Me
I'm a research scientist at [Google Research, Montreal](https://research.google/locations/montreal/), where I work on improving [machine translation](https://research.google/research-areas/machine-translation/). I'm generally interested in the **intersection of natural language processing (NLP) and machine learning**.

In my PhD (Heidelberg University, Germany) I investigated *how reinforcement learning algorithms can be used to turn weak supervision signals from users into meaningful updates for a machine translation system*.

One of my long-term goals is to make research in NLP more accessible, along multiple dimensions: 
1. **Underresourced NLP**: Produce research outcomes for underresourced languages, such that not only English-speaking users can benefit from the progress we're making in NLP. 
2. **Novices**: Reduce the entry burdens (in terms of coding and research practices)  for novices in the field, especially for new students or researchers from other related areas.
3. **Science outreach**: Get the general public, especially highschool students, more interested in research in machine learning to grow a better understanding of what our current methods look like and where their limitations are.

## News
- **Jan 2021**: 
  - [MeMentor session](https://mementor.net/#/session/600b178c6e2ab623d88be06c) on NLP Paper Writing. [Slides](https://github.com/juliakreutzer/juliakreutzer.github.io/blob/master/mementor_writing.pdf) 
- **Oct 2020**:
  - *Paper accepted at LoresMT 2020.* Preprint coming soon.
  - *Paper accepted at COLING 2020.* See [here](https://twitter.com/andre_niyongabo/status/1311719244703297536).
- **Sep 2020**:
  - *Paper accepted at EMNLP 2020.* Summarizing my internship project at Google, this paper analyses inference strategies for mask-based semi-autoregressive translation. Preprint [here](https://arxiv.org/pdf/2010.02352.pdf).
  - *Paper accepted at EMNLP Findings 2020.* Follow-up on the AfricaNLP paper about the [Masakhane community](https://www.masakhane.io/) and participatory research for low-resource machine translation. Preprint [here](https://arxiv.org/pdf/2010.02353.pdf).
  - My thesis is officially published, you can download it [here](https://doi.org/10.11588/heidok.00028862).
- **Jul 2020**:
  - [Invited talk](https://europe.naverlabs.com/research/seminars/reinforcement-learning-with-human-feedback-for-neural-machine-translation/) at NAVER LABS Europe: "Reinforcement learning with human feedback for neural machine translation".
- **Apr 2020**:
  - *Paper accepted at EAMT 2020.* This paper concludes my PhD thesis and empirically measures the trade-off between (human) feedback strength and model improvement in machine translation. Preprint [here](https://arxiv.org/pdf/2004.11222.pdf).
- **Mar 2020**: 
  - Successfully defended my PhD thesis.
  - *Papers accepted at the AfricaNLP workshop at ICLR.* 
     1. The first paper describes the efforts of the Masakhane community to build machine translation models for as many African languages as possible, by growing and fostering a distributed community of African researchers, students, and computer scientists (and more). Preprint [here](https://arxiv.org/abs/2003.11529). Check out the [Masakhane GitHub repo for the benchmarks](https://github.com/masakhane-io) for details. 
     2. The second paper describes the tuning of Transformer model depth for low-resource machine translation. Preprint [here](https://arxiv.org/pdf/2004.04418).
- **Feb 2020**: I joined the *Google Translate* team in Montreal <3.
- **Jan 2020**: Handed in my *thesis*, finally. 

## Publications
[Google scholar](https://scholar.google.com/citations?hl=en&user=j4cOSzAAAAAJ)

## Code
- [Joey NMT](https://github.com/joeynmt/joeynmt)

## Resources
- [HumanMT]()
- [Blog post on RL for NMT](https://www.cl.uni-heidelberg.de/statnlpgroup/blog/rl4nmt/)
- [Blog post on Joey NMT](https://www.cl.uni-heidelberg.de/statnlpgroup/blog/joey/)
