## About Me
I'm a research scientist at [Google Research, Montreal](https://research.google/locations/montreal/), where I work on improving [machine translation](https://research.google/research-areas/machine-translation/). I'm generally interested in the **intersection of natural language processing (NLP) and machine learning**.

In my PhD (Heidelberg University, Germany) I investigated *how reinforcement learning algorithms can be used to turn weak supervision signals from users into meaningful updates for a machine translation system*.

One of my long-term goals is to make research in NLP more accessible, along multiple dimensions: 
1. **Underresourced NLP**: Foster research for underresourced languages and by underrepresented groups, such that not only English-speaking users can benefit from the progress we're making in NLP. 
2. **Novices**: Reduce the entry burdens (in terms of coding and research practices)  for novices in the field, especially for new students or researchers from other related areas.
3. **Science outreach**: Get the general public, especially highschool students, more interested in research in machine learning to grow a better understanding of what our current methods look like and where their limitations are.

## News
- **Mar 2021**:
  - *2 accepted papers at AfricaNLP, and one at NAACL-HLT.* Details to come :)  
- **Feb 2021**:
  - [PyDataMTL talk] about "Learning to translate with JoeyNMT". [Slides](https://www.slideshare.net/juliaaakreutzer/learning-to-translate-with-joey-nmt-243430521) and [recording](https://www.youtube.com/watch?v=RG-yV5zgqjQ) are available. The notebook that was used in the demo is [here](https://github.com/joeynmt/joeynmt/blob/master/joey_demo.ipynb). It trains and evaluates a Transformer on Tatoeba data for a language pair of choice.
- **Jan 2021**: 
  - [MeMentor session](https://mementor.net/#/session/600b178c6e2ab623d88be06c) on NLP Paper Writing. Contact me for the slides.
- **Oct 2020**:
  - *Paper accepted at LoresMT 2020.* We built translation models for small, hand-aligned data for Bambara. Preprint [here](https://arxiv.org/abs/2011.05284).
  - *Paper accepted at COLING 2020.* This one is about building text classification corpora for Kinyarwanda and Kirundi, [as summarized by Andre](https://twitter.com/andre_niyongabo/status/1311719244703297536). Preprint [here](https://arxiv.org/abs/2010.12174).
- **Sep 2020**:
  - *Paper accepted at EMNLP 2020.* Summarizing my internship project at Google, this paper analyses inference strategies for mask-based semi-autoregressive translation. Preprint [here](https://arxiv.org/pdf/2010.02352.pdf).
  - *Paper accepted at EMNLP Findings 2020.* Follow-up on the AfricaNLP paper about the [Masakhane community](https://www.masakhane.io/) and participatory research for low-resource machine translation. Preprint [here](https://arxiv.org/pdf/2010.02353.pdf).
  - My thesis is officially published, you can download it [here](https://doi.org/10.11588/heidok.00028862).
- **Jul 2020**:
  - [Invited talk](https://europe.naverlabs.com/research/seminars/reinforcement-learning-with-human-feedback-for-neural-machine-translation/) at NAVER LABS Europe: "Reinforcement learning with human feedback for neural machine translation".
- **Apr 2020**:
  - *Paper accepted at EAMT 2020.* This paper concludes my PhD thesis and empirically measures the trade-off between (human) feedback strength and model improvement in machine translation. Preprint [here](https://arxiv.org/pdf/2004.11222.pdf).

<!--
- **Mar 2020**: 
  - Successfully defended my PhD thesis.
  - *Papers accepted at the AfricaNLP workshop at ICLR.* 
     1. The first paper describes the efforts of the Masakhane community to build machine translation models for as many African languages as possible, by growing and fostering a distributed community of African researchers, students, and computer scientists (and more). Preprint [here](https://arxiv.org/abs/2003.11529). Check out the [Masakhane GitHub repo for the benchmarks](https://github.com/masakhane-io) for details. 
     2. The second paper describes the tuning of Transformer model depth for low-resource machine translation. Preprint [here](https://arxiv.org/pdf/2004.04418).
- **Feb 2020**: I joined the *Google Translate* team in Montreal <3.
- **Jan 2020**: Handed in my *thesis*, finally. 
-->

## Publications
[Google scholar](https://scholar.google.com/citations?hl=en&user=j4cOSzAAAAAJ)

## Code
- [Joey NMT](https://github.com/joeynmt/joeynmt)

## Resources
- [HumanMT: Human feedback for MT from different interfaces](https://www.cl.uni-heidelberg.de/statnlpgroup/humanmt/)
- [Blog post on RL for NMT](https://www.cl.uni-heidelberg.de/statnlpgroup/blog/rl4nmt/)
- [Blog post on Joey NMT](https://www.cl.uni-heidelberg.de/statnlpgroup/blog/joey/)

## Contact
Email: <lowercase first letter of my first name><lowercase last name>@google.com
